# 初探大模型
AI自从1950开始被提出，到目前主要经历了四个发展阶段。AI研究主要有两个学派，一是链接主义还有一个是符号主义。
### 起源与发展
1. 解码注意力机制（Attention）
解码注意力的主要目的是在处理信息的时候能够聚焦于特定部分的技术。注意力机制通过计算不同信息片段的重要性，并据此分配不同的处理权重。
注意力机制实现步骤：
   - 确定信息片段 : 首先，模型需要识别出文本中的各个信息片段。
   - 计算权重 : 然后，模型会计算每个片段对于当前任务的重要性，这个过程称为“打分”。
   - 分配注意力: 根据打分结果，模型会给每个片段分配不同的注意力权重，重要的片段会获得更多的关注。
   - 整合信息: 最后，模型会综合考虑所有片段的信息，生成最终的输出。

2. Transformer - 变革里程碑
transformer是基于注意力机制的深度学习模型，并行计算能力强，长距离依赖关系捕捉能力强，训练速度快。
transformer的组成部分:
    - 编码器：编码器将输入序列转换为一个向量序列。
    - 解码器：解码器将编码生成的向量序列转换成输出序列。
    - 注意力机制： 注意力机制可以帮助模型关注输入序列中与当前词相关的部分。
    - 多头注意力机制：多头注意力机制可以帮助模型从多个角度关注输入序列中与当前词相关的部分。
3. GPT 与 Bert
GPT与Bert都是基于Transformer架构实现的大语言模型，但是存在一些关键的差异。
相同点:

    - 架构：GPT与Bert都是基于Transformer架构，改架构又编码器与解码器所组成。编码器负责处理文本，解码器负责生成文本。
    - 训练方式：都是通过无监督式学习来进行训练。
    - 应用：都是在自然语言处理方向上应用，比如翻译、文本生成、问答系统等

   不同点:
     
     - 训练方向：GPT是单向语言模型Bert是双向语言模型。
     - 预训练任务：GPT是预测下个文字或句子，

### 大语言模型家族

<!--stackedit_data:
eyJoaXN0b3J5IjpbMTYxNDg0NDM5MiwtMTU1MDcxODA5MywtMT
U1MDcxODA5MywtMTU4ODcwNTU0MCwtNTk4NjE3MzI4LC0xMzQ5
NTE2OTU4LC0xMTkzOTAwMTgzLC05NzU0ODI3NTMsLTkwMzk5NT
M5Myw4NDY2NTMzNTFdfQ==
-->